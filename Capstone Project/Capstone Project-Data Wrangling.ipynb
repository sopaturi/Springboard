{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "   This notebook will introduce the prediction problem that will be answered.  In addition, hypotheses will be made in order to aid in solving the problem as well as to start brewing insights on how to solve the problem.  The importance of solving the problem and how a company can use the resulting information will be looked at.  Finally, the data wrangling process will be conducted in order to clean the data set for future analysis. \n",
    "   \n",
    "   \n",
    "## Predicting the Likelihood of Acceptance for an H1B-LCA application\n",
    " \n",
    "An H1B visa is a non-immigrant visa that allows foreign born workers to enter the United States and work temporarily for up to three years with a possibility of extension for up to 6 years.  In order for a worker to receive an H1B visa, an employer must offer them a position and then submit an H1B visa application with the Department of Immigration.  H1B visas are commonly applied to by international students are who are looking to work after completing their education in the United States.\n",
    "\n",
    "A preliminary step before an H1B application can be filed is the submission of the LCA (Labor Condition Application) to the Department of Labor.  The LCA contains information about the job title being offered, duration of the job, whether job is full time, rate of pay, location of the job, and the prevailing wage in the area.  The purpose of the LCA is to bind the employer to agree to pay a fair wage and provide benefits to a foreign born worker that are equal to the prevailing wage and benefits at that occupation’s location.  In addition, the employer should provide the same working conditions to H1B applicants as other workers under the LCA application.  \n",
    "\n",
    "The data set contains one year worth of data from fiscal year 2017 and approximately 625,000 records of H1B-LCA application results.  I want to prevent employers from having their LCA application denied by identifying and communicating to the employer what the employment characteristics are for a high acceptance rate and low acceptance rate.  If the reason an application is denied is mainly due to the fields defined by the employer such as wage offered or worksite location, the employer will know what to change in their application in order to be certified.  The reason for H1B-LCA denial cannot be due to the employee because the application does not contain any information about the employee, only the employer.  In addition, I will be able to predict whether an employer’s prospective employee will have the LCA approved so that the employer can focus their time on changing characteristics about the job offered or the company so that an H1B-LCA is approved. \n",
    "\n",
    "My client is any company that wishes to hire a foreign born worker.  The client should care about H1B-LCA outcomes because they do not want to spend time filing and looking for candidates that will eventually not be certified with a successful H1B LCA application.  Based on my analysis, the employer will be able to find workers that are likely to be H1B-LCA certified by changing characterisitcs about the job offered to characteristics that have a high H1B-LCA certification rate.  \n",
    "\n",
    "Initial hypotheses for a H1B-LCA application denial include the employer name, the job title, whether the job is part time or not, the difference between the prevailing wage and the wage offered by the employer, whether the employer is H1-B dependent, whether the employer is a willful violator,  and whether the employer agreed to the labor condition agreement subsection on the application.  A large difference between prevailing wage and wage offered means the employer is not giving a fair wage which could cause the application to be denied.  An employer having too many H1-B employees, being a willful violator, or not agreeing to the labor condition agreement are all signs for an application possibly recieving a denied status. \n",
    "\n",
    "## Data Wrangling\n",
    "\n",
    "   The first step to examining the data involves data wrangling or the process of converting the data into a more valuable format that can be used for analysis.  The current data set was obtained from, https://www.foreignlaborcert.doleta.gov/performancedata.cfm.  The data set contains H1B-LCA application data from the 2017 fiscal year.  \n",
    "   \n",
    "   The data wrangling for this data set can be split into and defined by these four steps: deletion of unecessary columns, modification of rows that contain missing values, modification of rows with incorrect characters that do not represent the field correctly, and checking for outlier values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, the data set needs to be uploaded into a pandas dataframe so that data wrangling can start.  In order for pandas to read the .xslx file, the file was converted to a csv file and encoded to utf-8.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv('H1B1.csv', low_memory=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every column of the dataframe is shown below without hiding columns so that every column can be seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          CASE_NUMBER          CASE_STATUS CASE_SUBMITTED DECISION_DATE  \\\n",
      "0  I-200-16055-173457  CERTIFIED-WITHDRAWN      2/24/2016     10/1/2016   \n",
      "1  I-200-16064-557834  CERTIFIED-WITHDRAWN       3/4/2016     10/1/2016   \n",
      "\n",
      "  VISA_CLASS EMPLOYMENT_START_DATE EMPLOYMENT_END_DATE  \\\n",
      "0       H-1B             8/10/2016           8/10/2019   \n",
      "1       H-1B             8/16/2016           8/16/2019   \n",
      "\n",
      "            EMPLOYER_NAME EMPLOYER_BUSINESS_DBA     EMPLOYER_ADDRESS  \\\n",
      "0  DISCOVER PRODUCTS INC.                   NaN  2500 LAKE COOK ROAD   \n",
      "1        DFS SERVICES LLC                   NaN  2500 LAKE COOK ROAD   \n",
      "\n",
      "  EMPLOYER_CITY EMPLOYER_STATE EMPLOYER_POSTAL_CODE          EMPLOYER_COUNTRY  \\\n",
      "0    RIVERWOODS             IL                60015  UNITED STATES OF AMERICA   \n",
      "1    RIVERWOODS             IL                60015  UNITED STATES OF AMERICA   \n",
      "\n",
      "  EMPLOYER_PROVINCE EMPLOYER_PHONE EMPLOYER_PHONE_EXT  \\\n",
      "0               NaN     2244050900                NaN   \n",
      "1               NaN     2244050900                NaN   \n",
      "\n",
      "  AGENT_REPRESENTING_EMPLOYER AGENT_ATTORNEY_NAME AGENT_ATTORNEY_CITY  \\\n",
      "0                           Y     ELLSWORTH, CHAD            NEW YORK   \n",
      "1                           Y     ELLSWORTH, CHAD            NEW YORK   \n",
      "\n",
      "  AGENT_ATTORNEY_STATE                   JOB_TITLE SOC_CODE  \\\n",
      "0                   NY  ASSOCIATE DATA INTEGRATION  15-1121   \n",
      "1                   NY            SENIOR ASSOCIATE  15-2031   \n",
      "\n",
      "                       SOC_NAME NAICS_CODE  TOTAL_WORKERS  NEW_EMPLOYMENT  \\\n",
      "0     COMPUTER SYSTEMS ANALYSTS     522210              1               1   \n",
      "1  OPERATIONS RESEARCH ANALYSTS     522210              1               1   \n",
      "\n",
      "   CONTINUED_EMPLOYMENT  CHANGE_PREVIOUS_EMPLOYMENT  \\\n",
      "0                     0                           0   \n",
      "1                     0                           0   \n",
      "\n",
      "   NEW_CONCURRENT_EMPLOYMENT  CHANGE_EMPLOYER  AMENDED_PETITION  \\\n",
      "0                          0                0                 0   \n",
      "1                          0                0                 0   \n",
      "\n",
      "  FULL_TIME_POSITION PREVAILING_WAGE PW_UNIT_OF_PAY PW_WAGE_LEVEL PW_SOURCE  \\\n",
      "0                  Y       59,197.00           Year       Level I       OES   \n",
      "1                  Y       49,800.00           Year           NaN     Other   \n",
      "\n",
      "   PW_SOURCE_YEAR                                    PW_SOURCE_OTHER  \\\n",
      "0          2015.0                            OFLC ONLINE DATA CENTER   \n",
      "1          2015.0  TOWERS WATSON DATA SERVICES 2015 CSR PROFESSIO...   \n",
      "\n",
      "  WAGE_RATE_OF_PAY_FROM WAGE_RATE_OF_PAY_TO WAGE_UNIT_OF_PAY H1B_DEPENDENT  \\\n",
      "0             65,811.00           67,320.00             Year             N   \n",
      "1             53,000.00           57,200.00             Year             N   \n",
      "\n",
      "  WILLFUL_VIOLATOR SUPPORT_H1B LABOR_CON_AGREE  PUBLIC_DISCLOSURE_LOCATION  \\\n",
      "0                N         NaN               Y                         NaN   \n",
      "1                N         NaN               Y                         NaN   \n",
      "\n",
      "  WORKSITE_CITY WORKSITE_COUNTY WORKSITE_STATE WORKSITE_POSTAL_CODE  \\\n",
      "0    RIVERWOODS            LAKE             IL                60015   \n",
      "1    RIVERWOODS            LAKE             IL                60015   \n",
      "\n",
      "  ORIGINAL_CERT_DATE  \n",
      "0           3/1/2016  \n",
      "1           3/8/2016  \n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "print(df.head(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unecessary columns that do not help us with exploratory data analysis or answering the overall question of predicting the likelihood of an accepted application will be removed.  A justification will be provided for each field deletion so that columns with vital information are not mistakenly taken out.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CASE_NUMBER', 'CASE_STATUS', 'CASE_SUBMITTED', 'DECISION_DATE',\n",
       "       'VISA_CLASS', 'EMPLOYMENT_START_DATE', 'EMPLOYMENT_END_DATE',\n",
       "       'EMPLOYER_NAME', 'EMPLOYER_BUSINESS_DBA', 'EMPLOYER_ADDRESS',\n",
       "       'EMPLOYER_CITY', 'EMPLOYER_STATE', 'EMPLOYER_POSTAL_CODE',\n",
       "       'EMPLOYER_COUNTRY', 'EMPLOYER_PROVINCE', 'EMPLOYER_PHONE',\n",
       "       'EMPLOYER_PHONE_EXT', 'AGENT_REPRESENTING_EMPLOYER',\n",
       "       'AGENT_ATTORNEY_NAME', 'AGENT_ATTORNEY_CITY', 'AGENT_ATTORNEY_STATE',\n",
       "       'JOB_TITLE', 'SOC_CODE', 'SOC_NAME', 'NAICS_CODE', 'TOTAL_WORKERS',\n",
       "       'NEW_EMPLOYMENT', 'CONTINUED_EMPLOYMENT', 'CHANGE_PREVIOUS_EMPLOYMENT',\n",
       "       'NEW_CONCURRENT_EMPLOYMENT', 'CHANGE_EMPLOYER', 'AMENDED_PETITION',\n",
       "       'FULL_TIME_POSITION', 'PREVAILING_WAGE', 'PW_UNIT_OF_PAY',\n",
       "       'PW_WAGE_LEVEL', 'PW_SOURCE', 'PW_SOURCE_YEAR', 'PW_SOURCE_OTHER',\n",
       "       'WAGE_RATE_OF_PAY_FROM', 'WAGE_RATE_OF_PAY_TO', 'WAGE_UNIT_OF_PAY',\n",
       "       'H1B_DEPENDENT', 'WILLFUL_VIOLATOR', 'SUPPORT_H1B', 'LABOR_CON_AGREE',\n",
       "       'PUBLIC_DISCLOSURE_LOCATION', 'WORKSITE_CITY', 'WORKSITE_COUNTY',\n",
       "       'WORKSITE_STATE', 'WORKSITE_POSTAL_CODE', 'ORIGINAL_CERT_DATE'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns\n",
    "#df.drop(['CASE_NUMBER'], axis = 1, inplace = True) #The case number is an arbitrary string that does not affect application outcome\n",
    "df.drop(['DECISION_DATE'], axis = 1, inplace = True)  #The decision date is arrived at by the U.S. Department of Labor not by the applicant \n",
    "df.drop(['EMPLOYER_BUSINESS_DBA'], axis=1, inplace=True)  #The official employer is already listed, the unofficial name is unecessary\n",
    "df.drop(['EMPLOYER_PHONE'], axis=1, inplace=True) #The employer phone is not a variable that affects the application outcome\n",
    "df.drop(['EMPLOYER_PHONE_EXT'], axis=1, inplace=True) #The employer phone extension is not a variable that affects the application outcome\n",
    "df.drop(['AGENT_ATTORNEY_CITY'], axis=1, inplace=True) #The attorney's city is not of relevance to the LCA applicant\n",
    "df.drop(['AGENT_ATTORNEY_STATE'], axis=1, inplace=True) #The attorney's state is not relevant to the LCA applicant\n",
    "df.drop(['ORIGINAL_CERT_DATE'], axis=1, inplace=True) #This date is a decision made by the Department of Labor not applicant\n",
    "df.drop(['PUBLIC_DISCLOSURE_LOCATION'], axis=1, inplace=True) #Generic information that does not add any new information\n",
    "#df.drop(['PW_WAGE_LEVEL'], axis=1, inplace=True) #Wage is already listed, wage level does not provide any new information\n",
    "#df.drop(['PW_SOURCE_OTHER'], axis=1, inplace=True) #The source of prevailing wage is not affecting status of LCA application\n",
    "df.drop(['EMPLOYER_PROVINCE'], axis=1, inplace=True) #The province is incorrectly listed or not listed in many rows and provides no new information.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 624,650 rows in the data set.  A lot of these rows contain missing values.  When all rows with any missing values are deleted, only 98,155 rows remain.  Therefore, one should be cautious before deciding to remove rows based on missing values.  A missing value can be due to two reasons, the applicant filling out the H1B-LCA application chose not to input a value or the mechanism by which the data set was created contained scraping errors that resulted in only some values being picked up.  After reviewing the Office of Foreign Labor Certification’s iCERT Visa Portal System, where the application is filled out, it is clear based on the application instructions, that all the fields in the application are discretionary, meaning the applicant did not have to enter a value if they did not want to.  Because missing values are due to the optional nature of the application, having a missing value may affect application outcome.  Removing rows with missing values may skew the data set towards accepted applications because denied applications would be filtered out by removing rows with missing values in certain columns.  Therefore no columns in the H1B-LCA application data set will have the entries for their corresponding missing values removed.  In many cases, missing values will be imputed or labeled to signify there is exists a missing value at that position.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below finds all the columns that have an missing value.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['EMPLOYMENT_START_DATE',\n",
       " 'EMPLOYMENT_END_DATE',\n",
       " 'EMPLOYER_NAME',\n",
       " 'EMPLOYER_ADDRESS',\n",
       " 'EMPLOYER_CITY',\n",
       " 'EMPLOYER_STATE',\n",
       " 'EMPLOYER_POSTAL_CODE',\n",
       " 'EMPLOYER_COUNTRY',\n",
       " 'AGENT_REPRESENTING_EMPLOYER',\n",
       " 'JOB_TITLE',\n",
       " 'SOC_CODE',\n",
       " 'SOC_NAME',\n",
       " 'NAICS_CODE',\n",
       " 'FULL_TIME_POSITION',\n",
       " 'PREVAILING_WAGE',\n",
       " 'PW_UNIT_OF_PAY',\n",
       " 'PW_WAGE_LEVEL',\n",
       " 'PW_SOURCE',\n",
       " 'PW_SOURCE_YEAR',\n",
       " 'PW_SOURCE_OTHER',\n",
       " 'WAGE_RATE_OF_PAY_TO',\n",
       " 'WAGE_UNIT_OF_PAY',\n",
       " 'H1B_DEPENDENT',\n",
       " 'WILLFUL_VIOLATOR',\n",
       " 'SUPPORT_H1B',\n",
       " 'LABOR_CON_AGREE',\n",
       " 'WORKSITE_CITY',\n",
       " 'WORKSITE_COUNTY',\n",
       " 'WORKSITE_STATE',\n",
       " 'WORKSITE_POSTAL_CODE']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "columns=df.columns[df.isna().any()].tolist()\n",
    "columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before dealing with imputing a value for the missing values, the frequency of the ten most common values in a field with missing values along with the number of missing values will be computed.  This will serve to understand how many values in a field are null compared to the top values in the field.  In essence, it will be known which fields employers choose not fill in the most based on the null value count.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top values in the Employment Start Date field\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "9/1/2017     28425\n",
       "8/1/2017     12984\n",
       "9/15/2017    11067\n",
       "10/1/2017    10215\n",
       "7/1/2017      9110\n",
       "9/5/2017      8402\n",
       "9/12/2017     7238\n",
       "9/13/2017     6954\n",
       "8/15/2017     6739\n",
       "9/8/2017      6643\n",
       "9/16/2017     6568\n",
       "Name: EMPLOYMENT_START_DATE, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of null values in the employment start date column is: 29\n",
      "Top values in the Employment End Date field\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8/31/2020    17753\n",
       "9/1/2020     13063\n",
       "7/31/2020     9137\n",
       "9/14/2020     9038\n",
       "9/15/2020     8333\n",
       "8/15/2020     7721\n",
       "9/30/2020     7432\n",
       "9/12/2020     6924\n",
       "9/5/2020      6839\n",
       "9/4/2020      6742\n",
       "6/30/2020     6556\n",
       "Name: EMPLOYMENT_END_DATE, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of null values in the employment end date column is: 30\n",
      "Top values in the Employer Name field\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "INFOSYS LIMITED                      20587\n",
       "TATA CONSULTANCY SERVICES LIMITED    13529\n",
       "CAPGEMINI AMERICA INC                 9604\n",
       "IBM INDIA PRIVATE LIMITED             8734\n",
       "TECH MAHINDRA (AMERICAS),INC.         7543\n",
       "DELOITTE CONSULTING LLP               7179\n",
       "ACCENTURE LLP                         6690\n",
       "ERNST & YOUNG U.S. LLP                6096\n",
       "WIPRO LIMITED                         5177\n",
       "GOOGLE INC.                           5074\n",
       "MICROSOFT CORPORATION                 5005\n",
       "Name: EMPLOYER_NAME, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of null values in the employmer name column is: 56\n",
      "Top values in the Employer Address field\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6100 TENNYSON PARKWAY          20748\n",
       "9201 CORPORATE BOULEVARD       13242\n",
       "1700 MARKET STREET              9775\n",
       "333 WEST WACKER DRIVE           9714\n",
       "3039 CORNWALLIS ROAD            8816\n",
       "4965 PRESTON PARK BOULEVARD     8291\n",
       "200 PLAZA DRIVE                 6239\n",
       "2 TOWER CENTER BLVD             5820\n",
       "161 N. CLARK ST.                5760\n",
       "525 EAST BIG BEAVER ROAD        5590\n",
       "1600 AMPHITHEATRE PARKWAY       5356\n",
       "Name: EMPLOYER_ADDRESS, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of null values in the employmer address column is: 7\n",
      "Top values in the Employer City field\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PLANO            34320\n",
       "NEW YORK         31022\n",
       "CHICAGO          25581\n",
       "ROCKVILLE        14499\n",
       "EDISON           12670\n",
       "PHILADELPHIA     12057\n",
       "DURHAM           11638\n",
       "SAN FRANCISCO    11199\n",
       "TROY             10938\n",
       "IRVING           10445\n",
       "HOUSTON           9795\n",
       "Name: EMPLOYER_CITY, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of null values in the employmer city column is: 15\n",
      "Top values in the Employer State field\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CA    105701\n",
       "TX     87664\n",
       "NJ     76061\n",
       "NY     44097\n",
       "IL     41296\n",
       "MI     27320\n",
       "PA     25564\n",
       "MA     23383\n",
       "MD     21394\n",
       "WA     20786\n",
       "VA     19374\n",
       "Name: EMPLOYER_STATE, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of null values in the employmer state column is: 18\n",
      "Top values in the Employer Postal Code field\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "75024    22815\n",
       "20850    14287\n",
       "27709    11473\n",
       "60606    11023\n",
       "19103    10411\n",
       "75093     9269\n",
       "08817     9169\n",
       "94043     7837\n",
       "48083     7759\n",
       "95054     7594\n",
       "60601     7527\n",
       "Name: EMPLOYER_POSTAL_CODE, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of null values in the employmer postal code column is: 18\n",
      "Top values in the Employer Country field\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "UNITED STATES OF AMERICA    528132\n",
       "CANADA                           7\n",
       "AUSTRALIA                        2\n",
       "CHINA                            1\n",
       "CAMBODIA                         1\n",
       "Name: EMPLOYER_COUNTRY, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of null values in the employmer country column is: 96507\n",
      "Top values in the Attorney field\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Y    351346\n",
       "N    176798\n",
       "Name: AGENT_REPRESENTING_EMPLOYER, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of null values in the attorney name column is: 96506\n",
      "Top values in the Job Title field\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PROGRAMMER ANALYST          47151\n",
       "SOFTWARE ENGINEER           27950\n",
       "SOFTWARE DEVELOPER          16107\n",
       "SYSTEMS ANALYST             10798\n",
       "BUSINESS ANALYST             7410\n",
       "COMPUTER PROGRAMMER          7303\n",
       "SENIOR SOFTWARE ENGINEER     6983\n",
       "COMPUTER SYSTEMS ANALYST     6555\n",
       "DEVELOPER                    5835\n",
       "ASSISTANT PROFESSOR          4957\n",
       "PROJECT MANAGER              4262\n",
       "Name: JOB_TITLE, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of null values in the job title column is: 5\n",
      "Top values in the Occupation Code field\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SOFTWARE DEVELOPERS, APPLICATIONS              124507\n",
       "COMPUTER SYSTEMS ANALYSTS                       93611\n",
       "COMPUTER PROGRAMMERS                            66603\n",
       "COMPUTER OCCUPATIONS, ALL OTHER                 53440\n",
       "SOFTWARE DEVELOPERS, SYSTEMS SOFTWARE           18401\n",
       "COMPUTER SYSTEMS ANALYST                        16902\n",
       "MANAGEMENT ANALYSTS                             13092\n",
       "ACCOUNTANTS AND AUDITORS                        11889\n",
       "NETWORK AND COMPUTER SYSTEMS ADMINISTRATORS     10817\n",
       "MECHANICAL ENGINEERS                             9886\n",
       "FINANCIAL ANALYSTS                               9633\n",
       "Name: SOC_NAME, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of null values in the occupation code column is: 3\n",
      "Top values in the Industry Code field\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "541511    232810\n",
       "541512     33909\n",
       "611310     25534\n",
       "54151      20914\n",
       "5416       19831\n",
       "541519     14205\n",
       "541330     13307\n",
       "54161       8508\n",
       "541211      7851\n",
       "622110      7304\n",
       "541611      7279\n",
       "Name: NAICS_CODE, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of null values in the industry code column is: 7\n",
      "Top values in the Full Time Position field\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Y    610769\n",
       "N     13876\n",
       "Name: FULL_TIME_POSITION, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of null values in the full time position column is: 5\n",
      "Top values in the Prevailing Wage field\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "68,827.00     3928\n",
       "55,910.00     3918\n",
       "62,754.00     3661\n",
       "64,813.00     3376\n",
       "70,221.00     3334\n",
       "46,301.00     2771\n",
       "75,795.00     2728\n",
       "106,101.00    2370\n",
       "90,813.00     2329\n",
       "84,864.00     2183\n",
       "71,781.00     2142\n",
       "Name: PREVAILING_WAGE, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of null values in the prevailing wage column is: 1\n",
      "Top values in the PW Unit field\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Year         585301\n",
       "Hour          38779\n",
       "Month           327\n",
       "Week            140\n",
       "Bi-Weekly        57\n",
       "Name: PW_UNIT_OF_PAY, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of null values in the PW unit column is: 46\n",
      "Top values in the PW Source field\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OES      504806\n",
       "Other    115702\n",
       "CBA        4053\n",
       "DBA          22\n",
       "SCA          21\n",
       "Name: PW_SOURCE, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of null values in the PW source column is: 46\n",
      "Top values in the PW Source Year field\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2016.0    437864\n",
       "2017.0    157657\n",
       "2015.0     17413\n",
       "2014.0      8236\n",
       "2013.0      2992\n",
       "2011.0       241\n",
       "2012.0       108\n",
       "2009.0        30\n",
       "2010.0        19\n",
       "2008.0        18\n",
       "2007.0        13\n",
       "2001.0         5\n",
       "0.0            2\n",
       "2000.0         1\n",
       "1999.0         1\n",
       "1998.0         1\n",
       "1993.0         1\n",
       "1985.0         1\n",
       "1.0            1\n",
       "15.0           1\n",
       "Name: PW_SOURCE_YEAR, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of null values in the PW source year column is: 45\n",
      "Top values in the Maximum Wage Offered field\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.00          488992\n",
       "87,000.00       4406\n",
       "100,000.00      3758\n",
       "120,000.00      3557\n",
       "90,000.00       2949\n",
       "110,000.00      2896\n",
       "94,500.00       2883\n",
       "88,200.00       2601\n",
       "70,000.00       2274\n",
       "95,000.00       2155\n",
       "75,000.00       2128\n",
       "Name: WAGE_RATE_OF_PAY_TO, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of null values in the maximum wage offered column is: 1\n",
      "Top values in the Wage Offered Unit field\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Year         585417\n",
       "Hour          38405\n",
       "Month           502\n",
       "Week            188\n",
       "Bi-Weekly       130\n",
       "Name: WAGE_UNIT_OF_PAY, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of null values in the wage offered unit column is: 8\n",
      "Top values in the H1B Dependent field\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "N    377051\n",
       "Y    233857\n",
       "Name: H1B_DEPENDENT, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of null values in the H1B dependent column is: 13742\n",
      "Top values in the Willful Violator field\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "N    610428\n",
       "Y       477\n",
       "Name: WILLFUL_VIOLATOR, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of null values in the willful violator column is: 13745\n",
      "Top values in the Support H1B field\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Y    231915\n",
       "N      8561\n",
       "Name: SUPPORT_H1B, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of null values in the support H1B column is: 384174\n",
      "Top values in the Labor Condition Agreement field\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Y    233715\n",
       "N     11260\n",
       "Name: LABOR_CON_AGREE, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of null values in the labor condition agreement column is: 379675\n",
      "Top values in the Worksite City field\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "NEW YORK         37722\n",
       "SAN FRANCISCO    16145\n",
       "HOUSTON          13416\n",
       "CHICAGO          11757\n",
       "ATLANTA          11115\n",
       "SAN JOSE         10916\n",
       "IRVING            8818\n",
       "SEATTLE           8630\n",
       "SUNNYVALE         8282\n",
       "DALLAS            8056\n",
       "MOUNTAIN VIEW     7531\n",
       "Name: WORKSITE_CITY, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of null values in the worksite city column is: 13\n",
      "Top values in the Worksite County field\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SANTA CLARA      41862\n",
       "NEW YORK         37684\n",
       "KING             22563\n",
       "DALLAS           21697\n",
       "MIDDLESEX        21469\n",
       "COOK             17641\n",
       "LOS ANGELES      17411\n",
       "SAN FRANCISCO    15678\n",
       "HARRIS           14040\n",
       "FULTON           13151\n",
       "SUFFOLK           8796\n",
       "Name: WORKSITE_COUNTY, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of null values in the worksite county column is: 1179\n",
      "Top values in the Worksite State field\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CA    119409\n",
       "TX     66163\n",
       "NY     54974\n",
       "NJ     42571\n",
       "IL     32082\n",
       "WA     25476\n",
       "MA     25016\n",
       "GA     22521\n",
       "FL     21815\n",
       "PA     21600\n",
       "MI     20222\n",
       "Name: WORKSITE_STATE, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of null values in the worksite state column is: 9\n",
      "Top values in the Worksite Postal Code field\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "94043    6683\n",
       "98052    6487\n",
       "94105    6065\n",
       "95054    4810\n",
       "95134    4304\n",
       "94085    4037\n",
       "10036    3892\n",
       "75024    3658\n",
       "60606    3593\n",
       "75038    3174\n",
       "10017    2830\n",
       "Name: WORKSITE_POSTAL_CODE, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of null values in the worksite postal code is: 19\n"
     ]
    }
   ],
   "source": [
    "print('Top values in the Employment Start Date field')\n",
    "df['EMPLOYMENT_START_DATE'].value_counts()[:11]\n",
    "print('The number of null values in the employment start date column is: ' + str(df['EMPLOYMENT_START_DATE'].isnull().sum()))\n",
    "\n",
    "print('Top values in the Employment End Date field')\n",
    "df['EMPLOYMENT_END_DATE'].value_counts()[:11]\n",
    "print('The number of null values in the employment end date column is: ' + str(df['EMPLOYMENT_END_DATE'].isnull().sum()))\n",
    "\n",
    "print('Top values in the Employer Name field')\n",
    "df['EMPLOYER_NAME'].value_counts()[:11]\n",
    "print('The number of null values in the employmer name column is: ' + str(df['EMPLOYER_NAME'].isnull().sum()))\n",
    "\n",
    "print('Top values in the Employer Address field')\n",
    "df['EMPLOYER_ADDRESS'].value_counts()[:11]\n",
    "print('The number of null values in the employmer address column is: ' + str(df['EMPLOYER_ADDRESS'].isnull().sum()))\n",
    "\n",
    "print('Top values in the Employer City field')\n",
    "df['EMPLOYER_CITY'].value_counts()[:11]\n",
    "print('The number of null values in the employmer city column is: ' + str(df['EMPLOYER_CITY'].isnull().sum()))\n",
    "\n",
    "print('Top values in the Employer State field')\n",
    "df['EMPLOYER_STATE'].value_counts()[:11]\n",
    "print('The number of null values in the employmer state column is: ' + str(df['EMPLOYER_STATE'].isnull().sum()))\n",
    "\n",
    "print('Top values in the Employer Postal Code field')\n",
    "df['EMPLOYER_POSTAL_CODE'].value_counts()[:11]\n",
    "print('The number of null values in the employmer postal code column is: ' + str(df['EMPLOYER_POSTAL_CODE'].isnull().sum()))\n",
    "\n",
    "print('Top values in the Employer Country field')\n",
    "df['EMPLOYER_COUNTRY'].value_counts()[:11]\n",
    "print('The number of null values in the employmer country column is: ' + str(df['EMPLOYER_COUNTRY'].isnull().sum()))\n",
    "\n",
    "print('Top values in the Attorney field')\n",
    "df['AGENT_REPRESENTING_EMPLOYER'].value_counts()[:11]\n",
    "print('The number of null values in the attorney name column is: ' + str(df['AGENT_REPRESENTING_EMPLOYER'].isnull().sum()))\n",
    "\n",
    "print('Top values in the Job Title field')\n",
    "df['JOB_TITLE'].value_counts()[:11]\n",
    "print('The number of null values in the job title column is: ' + str(df['JOB_TITLE'].isnull().sum()))\n",
    "\n",
    "print('Top values in the Occupation Code field')\n",
    "df['SOC_NAME'].value_counts()[:11]\n",
    "print('The number of null values in the occupation code column is: ' + str(df['SOC_NAME'].isnull().sum()))\n",
    "\n",
    "print('Top values in the Industry Code field')\n",
    "df['NAICS_CODE'].value_counts()[:11]\n",
    "print('The number of null values in the industry code column is: ' + str(df['NAICS_CODE'].isnull().sum()))\n",
    "\n",
    "print('Top values in the Full Time Position field')\n",
    "df['FULL_TIME_POSITION'].value_counts()[:11]\n",
    "print('The number of null values in the full time position column is: ' + str(df['FULL_TIME_POSITION'].isnull().sum()))\n",
    "\n",
    "print('Top values in the Prevailing Wage field')\n",
    "df['PREVAILING_WAGE'].value_counts()[:11]\n",
    "print('The number of null values in the prevailing wage column is: ' + str(df['PREVAILING_WAGE'].isnull().sum()))\n",
    "\n",
    "print('Top values in the PW Unit field')\n",
    "df['PW_UNIT_OF_PAY'].value_counts()[:11]\n",
    "print('The number of null values in the PW unit column is: ' + str(df['PW_UNIT_OF_PAY'].isnull().sum()))\n",
    "\n",
    "print('Top values in the PW Source field')\n",
    "df['PW_SOURCE'].value_counts()[:11]\n",
    "print('The number of null values in the PW source column is: ' + str(df['PW_SOURCE'].isnull().sum()))\n",
    "\n",
    "print('Top values in the PW Source Year field')\n",
    "df['PW_SOURCE_YEAR'].value_counts()\n",
    "print('The number of null values in the PW source year column is: ' + str(df['PW_SOURCE_YEAR'].isnull().sum()))\n",
    "\n",
    "print('Top values in the Maximum Wage Offered field')\n",
    "df['WAGE_RATE_OF_PAY_TO'].value_counts()[:11]\n",
    "print('The number of null values in the maximum wage offered column is: ' + str(df['WAGE_RATE_OF_PAY_TO'].isnull().sum()))\n",
    "\n",
    "print('Top values in the Wage Offered Unit field')\n",
    "df['WAGE_UNIT_OF_PAY'].value_counts()[:11]\n",
    "print('The number of null values in the wage offered unit column is: ' + str(df['WAGE_UNIT_OF_PAY'].isnull().sum()))\n",
    "\n",
    "print('Top values in the H1B Dependent field')\n",
    "df['H1B_DEPENDENT'].value_counts()[:11]\n",
    "print('The number of null values in the H1B dependent column is: ' + str(df['H1B_DEPENDENT'].isnull().sum()))\n",
    "\n",
    "print('Top values in the Willful Violator field')\n",
    "df['WILLFUL_VIOLATOR'].value_counts()[:11]\n",
    "print('The number of null values in the willful violator column is: ' + str(df['WILLFUL_VIOLATOR'].isnull().sum()))\n",
    "\n",
    "print('Top values in the Support H1B field')\n",
    "df['SUPPORT_H1B'].value_counts()[:11]\n",
    "print('The number of null values in the support H1B column is: ' + str(df['SUPPORT_H1B'].isnull().sum()))\n",
    "\n",
    "print('Top values in the Labor Condition Agreement field')\n",
    "df['LABOR_CON_AGREE'].value_counts()[:11]\n",
    "print('The number of null values in the labor condition agreement column is: ' + str(df['LABOR_CON_AGREE'].isnull().sum()))\n",
    "\n",
    "print('Top values in the Worksite City field')\n",
    "df['WORKSITE_CITY'].value_counts()[:11]\n",
    "print('The number of null values in the worksite city column is: ' + str(df['WORKSITE_CITY'].isnull().sum()))\n",
    "\n",
    "print('Top values in the Worksite County field')\n",
    "df['WORKSITE_COUNTY'].value_counts()[:11]\n",
    "print('The number of null values in the worksite county column is: ' + str(df['WORKSITE_COUNTY'].isnull().sum()))\n",
    "\n",
    "print('Top values in the Worksite State field')\n",
    "df['WORKSITE_STATE'].value_counts()[:11]\n",
    "print('The number of null values in the worksite state column is: ' + str(df['WORKSITE_STATE'].isnull().sum()))\n",
    "\n",
    "print('Top values in the Worksite Postal Code field')\n",
    "df['WORKSITE_POSTAL_CODE'].value_counts()[:11]\n",
    "print('The number of null values in the worksite postal code is: ' + str(df['WORKSITE_POSTAL_CODE'].isnull().sum()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is known now how many missing values there are per field.  Now, it must be decided on how to replace these missing values.  If a missing value is a categorical variable, the missing value will be replaced with the string 'None'.  If the missing value is a numerical variable, the missing value will be replaced with an imputed value.  The missing categorical variables will first be replaced with 'None' in the cell below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['EMPLOYMENT_START_DATE'].fillna(value='None', inplace=True)\n",
    "df['EMPLOYMENT_END_DATE'].fillna(value='None', inplace=True)\n",
    "df['EMPLOYER_NAME'].fillna(value='None', inplace=True)\n",
    "df['EMPLOYER_ADDRESS'].fillna(value='None', inplace=True)\n",
    "df['EMPLOYER_CITY'].fillna(value='None', inplace=True)\n",
    "df['EMPLOYER_STATE'].fillna(value='None', inplace=True)\n",
    "df['EMPLOYER_POSTAL_CODE'].fillna(value='None', inplace=True)\n",
    "df['EMPLOYER_COUNTRY'].fillna(value='None', inplace=True)\n",
    "df['AGENT_REPRESENTING_EMPLOYER'].fillna(value='None', inplace=True)\n",
    "df['AGENT_ATTORNEY_NAME'].fillna(value='None', inplace=True)\n",
    "df['JOB_TITLE'].fillna(value='None', inplace=True)\n",
    "df['SOC_NAME'].fillna(value='None', inplace=True)\n",
    "df['NAICS_CODE'].fillna(value='None', inplace=True)\n",
    "df['FULL_TIME_POSITION'].fillna(value='None', inplace=True)\n",
    "\n",
    "df['PW_UNIT_OF_PAY'].fillna(value='None', inplace=True)\n",
    "df['PW_SOURCE'].fillna(value='None', inplace=True)\n",
    "df['PW_SOURCE_YEAR'].fillna(value='None', inplace=True)\n",
    "df['PW_WAGE_LEVEL'].fillna(value='None', inplace=True)\n",
    "df['PW_SOURCE_OTHER'].fillna(value='None', inplace=True)\n",
    "df['WAGE_UNIT_OF_PAY'].fillna(value='None', inplace=True)\n",
    "df['H1B_DEPENDENT'].fillna(value='None', inplace=True)\n",
    "df['WILLFUL_VIOLATOR'].fillna(value='None', inplace=True)\n",
    "df['SUPPORT_H1B'].fillna(value='None', inplace=True)\n",
    "df['LABOR_CON_AGREE'].fillna(value='None', inplace=True)\n",
    "df['WORKSITE_CITY'].fillna(value='None', inplace=True)\n",
    "df['WORKSITE_COUNTY'].fillna(value='None', inplace=True)\n",
    "df['WORKSITE_STATE'].fillna(value='None', inplace=True)\n",
    "df['WORKSITE_POSTAL_CODE'].fillna(value='None', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the entries with missing values in categorical fields have been replaced, values that are irregularities need to be replaced with more understandable notation.  For example in the AGENT_ATTORNEY_NAME field, a comma is in many rows that do not have an attorney name.  The comma can be replaced with the value None so that it is easily identifiable if an application had an attorney or not.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['AGENT_ATTORNEY_NAME'] = df['AGENT_ATTORNEY_NAME'].map({',': 'None'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, the industry from which the job is in is represented by a NAICS_CODE.  The NAICS code needs to be converted to an industry name.  The following code cell retrieves the corresponding industry name from the 2012_NAICS_Structure.csv and adds codes with titles to a dictionary.  NAICS code values are first replaced in the dataframe with industry names then the codes without an entry in the dictionary are replaced with the string 'None'.  Replacing the numerical codes with industry names is more readable and easy to understand.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-058eb0c62150>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mnaics_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNAICS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'2012 NAICS Code'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNAICS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'2012 NAICS Title'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m     \u001b[0;31m#Iterate through each row in the dataframe\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mcode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'NAICS_CODE'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m#Focus on the NAICS_CODE column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mcode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36miterrows\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    773\u001b[0m         \u001b[0mklass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor_sliced\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    774\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 775\u001b[0;31m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    776\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    274\u001b[0m                                        raise_cast_failure=True)\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSingleBlockManager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfastpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0mgeneric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNDFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfastpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, block, axis, do_integrity_check, fastpath)\u001b[0m\n\u001b[1;32m   4675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4676\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBlock\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4677\u001b[0;31m             \u001b[0mblock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplacement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mslice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4678\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4679\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36mmake_block\u001b[0;34m(values, placement, klass, ndim, dtype, fastpath)\u001b[0m\n\u001b[1;32m   3203\u001b[0m                      placement=placement, dtype=dtype)\n\u001b[1;32m   3204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3205\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplacement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplacement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3207\u001b[0m \u001b[0;31m# TODO: flexible with index=None and/or items=None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, values, placement, ndim)\u001b[0m\n\u001b[1;32m   2301\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2302\u001b[0m         super(ObjectBlock, self).__init__(values, ndim=ndim,\n\u001b[0;32m-> 2303\u001b[0;31m                                           placement=placement)\n\u001b[0m\u001b[1;32m   2304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2305\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/internals.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, values, placement, ndim)\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m         if (self._validate_ndim and self.ndim and\n\u001b[0m\u001b[1;32m    122\u001b[0m                 len(self.mgr_locs) != len(self.values)):\n\u001b[1;32m    123\u001b[0m             raise ValueError(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "NAICS=pd.read_csv('2012_NAICS_Structure.csv')\n",
    "NAICS.drop(['2012 NAICS Structure'], axis=1, inplace=True) \n",
    "NAICS=NAICS.dropna(subset = ['Unnamed: 1', 'Unnamed: 2'])\n",
    "NAICS.columns=['2012 NAICS Code', '2012 NAICS Title']\n",
    "NAICS=NAICS.loc[2:]\n",
    "\n",
    "naics_dict = dict(zip(NAICS['2012 NAICS Code'], NAICS['2012 NAICS Title']))\n",
    "\n",
    "for index, row in df.iterrows():     #Iterate through each row in the dataframe\n",
    "        code=(row['NAICS_CODE'])   #Focus on the NAICS_CODE column\n",
    "        code=str(code)\n",
    "        if code not in naics_dict:  #check if code is in dictionary\n",
    "            df.at[index, 'NAICS_CODE'] = 'None'\n",
    "        else:\n",
    "            df.at[index, 'NAICS_CODE'] = naics_dict[code]\n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the case status column will be examined in order to remove irrelevant rows.  The case status column identifies whether or not the H1B-LCA application was certified, denied, withdrawn, or certified-withdraw.  Because the goal is to predict the likelihood of an applicant receiving a certified status, the rows containing applications that were withdrawn can be removed as they give no insight on whether an application was certified or not.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.CASE_STATUS != 'WITHDRAWN']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A possible reason for the declined H1B-LCA application is the difference between the prevailing wage and the wage offered by the employer.  If the wage difference is large, it may be a reason for the application receiving a rejection because a lower wage than the prevailing wage is considered unfair.  The units for the prevailing wage and the offered wage may be in different units such as monthly, hourly, and yearly.  The following script converts the units and the monetary value of the prevailing wage as well as the wage offered by the employer to a yearly unit and the monetary value corresponding to an annual wage.  \n",
    "\n",
    "Another reason the application could be rejected is that prevailing wage listed is not correctly listed.  By converting all of the salaries to annual salaries, the salaries can be compared to a database with prevailing wage values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    row['PREVAILING_WAGE']=str(row['PREVAILING_WAGE']).replace(',','')\n",
    "    row['WAGE_RATE_OF_PAY_FROM']=str(row['WAGE_RATE_OF_PAY_FROM']).replace(',','')\n",
    "    if (row['PW_UNIT_OF_PAY']=='Hour'):\n",
    "        df.at[index, 'PW_UNIT_OF_PAY'] = 'Year'\n",
    "        df.at[index, 'PREVAILING_WAGE'] = float(row['PREVAILING_WAGE'])*(2080)\n",
    "    if (row['WAGE_UNIT_OF_PAY']=='Hour'):\n",
    "        df.at[index, 'WAGE_UNIT_OF_PAY']= 'Year'\n",
    "        df.at[index, 'WAGE_RATE_OF_PAY_FROM'] = float(row['WAGE_RATE_OF_PAY_FROM'])*(2080) \n",
    "    \n",
    "    if (row['PW_UNIT_OF_PAY']=='Week'):\n",
    "        df.at[index, 'PW_UNIT_OF_PAY'] = 'Year'\n",
    "        df.at[index, 'PREVAILING_WAGE'] = float(row['PREVAILING_WAGE'])*(2080/40)\n",
    "    if (row['WAGE_UNIT_OF_PAY']=='Week'):\n",
    "        df.at[index, 'WAGE_UNIT_OF_PAY']= 'Year'\n",
    "        df.at[index, 'WAGE_RATE_OF_PAY_FROM'] = float(row['WAGE_RATE_OF_PAY_FROM'])*(2080/40)  \n",
    "    if (row['PW_UNIT_OF_PAY']=='Month'):\n",
    "        df.at[index, 'PW_UNIT_OF_PAY'] = 'Year'\n",
    "        df.at[index, 'PREVAILING_WAGE'] = float(row['PREVAILING_WAGE'])*(12)\n",
    "    if (row['WAGE_UNIT_OF_PAY']=='Month'):\n",
    "        df.at[index, 'WAGE_UNIT_OF_PAY']= 'Year'\n",
    "        df.at[index, 'WAGE_RATE_OF_PAY_FROM'] = float(row['WAGE_RATE_OF_PAY_FROM'])*(12)\n",
    "    if (row['PW_UNIT_OF_PAY']=='Bi-Weekly'):\n",
    "        df.at[index, 'PW_UNIT_OF_PAY'] = 'Year'\n",
    "        df.at[index, 'PREVAILING_WAGE'] = float(row['PREVAILING_WAGE'])*(2080/80)\n",
    "    if (row['WAGE_UNIT_OF_PAY']=='Bi-Weekly'):\n",
    "        df.at[index, 'WAGE_UNIT_OF_PAY'] = 'Year'\n",
    "        df.at[index, 'WAGE_RATE_OF_PAY_FROM'] = float(row['WAGE_RATE_OF_PAY_FROM'])*(2080/80)\n",
    "    if (row['PW_UNIT_OF_PAY']=='Year')& (row['WAGE_UNIT_OF_PAY']=='Year'):\n",
    "        continue    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to see if there is a discrepancy between the prevailing wage and wage given by the employer, a calculated field is created below that represents the difference between the prevailing wage and wage given by the employer.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    df.at[index, 'WAGE_RATE_OF_PAY_FROM'] = float(str(row['WAGE_RATE_OF_PAY_FROM']).replace(',',''))\n",
    "    df.at[index, 'PREVAILING_WAGE']=float(str(row['PREVAILING_WAGE']).replace(',',''))\n",
    "df['WAGE_DIFFERENCE'] = df.WAGE_RATE_OF_PAY_FROM.astype(float)-df.PREVAILING_WAGE.astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another aspect of data wrangling involves finding the outliers in a field that may lead to faulty analysis later on.  In order to make sure the prevailing wage column and the wage rate offered by the employer contain values that are valid, a scatter plot can be constructed to make sure the salaries fall within a reasonable range.  \n",
    "\n",
    "The lists for graphing wage offered vs. prevailing wage by different units of time are created in the code below.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pw_year=[]\n",
    "wage_year=[]\n",
    "pw_month=[]\n",
    "wage_month=[]\n",
    "pw_week=[]\n",
    "wage_week=[]\n",
    "pw_hour=[]\n",
    "wage_hour=[]\n",
    "pw_biweekly=[]\n",
    "wage_biweekly=[]\n",
    "for index, row in df.iterrows():\n",
    "    if (row['PW_UNIT_OF_PAY']=='Year') & (row['WAGE_UNIT_OF_PAY']=='Year'):   #Wage w/ units Year\n",
    "        pw_year.append(row['PREVAILING_WAGE'])\n",
    "        wage_year.append(row['WAGE_RATE_OF_PAY_FROM'])\n",
    "    if (row['PW_UNIT_OF_PAY']=='Month') & (row['WAGE_UNIT_OF_PAY']=='Month'): #Wage w/ units Month\n",
    "        pw_month.append(row['PREVAILING_WAGE'])\n",
    "        wage_month.append(row['WAGE_RATE_OF_PAY_FROM'])\n",
    "    if (row['PW_UNIT_OF_PAY']=='Week') & (row['WAGE_UNIT_OF_PAY']=='Week'):  #Wage w/ units Week\n",
    "        pw_week.append(row['PREVAILING_WAGE'])\n",
    "        wage_week.append(row['WAGE_RATE_OF_PAY_FROM'])\n",
    "    if (row['PW_UNIT_OF_PAY']=='Hour') & (row['WAGE_UNIT_OF_PAY']=='Hour'):  #Week w/ units Hour\n",
    "        pw_hour.append(row['PREVAILING_WAGE'])\n",
    "        wage_hour.append(row['WAGE_RATE_OF_PAY_FROM'])\n",
    "    if (row['PW_UNIT_OF_PAY']=='Bi-Weekly') & (row['WAGE_UNIT_OF_PAY']=='Bi-Weekly'):  #Week w/ units Bi-Weekly\n",
    "        pw_biweekly.append(row['PREVAILING_WAGE'])\n",
    "        wage_biweekly.append(row['WAGE_RATE_OF_PAY_FROM'])\n",
    "    \n",
    "    \n",
    "%store pw_year\n",
    "%store wage_year\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the WAGE_RATE_OF_PAY_TO field, many cells have the value 0.00, 448,992 cells have this value to be exact.  This field represents the maximum wage offered by the employer.  0.00 cannot be replaced with 'None' as this field is a numerical variable that cannot be replaced with a categorical variable value.  Instead, the value 0.00 can be repaced with the value in the wage offered column because the employer did not offer a maximum wage and the maximum wage can be substituted with wage offered.  Many employers may not offer a maximum wage because they do not want wage to have a range of values, therefore the wage offered will be used in the cases where WAGE_RATE_OF_PAY_TO is 0.00 or null.  \n",
    "\n",
    "There is one more field left that is numerical.  It is the prevailing wage field and it only has one entry with a missing value.  The value cannot be replaced with 'None' because the field is a numerical variable.  Therefore, the missing value is replaced with '0.00' because this application was denied and all the other applications that had a prevailing wage of '0.00' the application denied.  By replacing the missing value with '0.00' the application is grouped with other denied applications and maintains its own denied identity.   \n",
    "\n",
    "Two missing value indicator columns were created in the cell below so that it known if the WAGE_RATE_OF_PAY_TO or the PREVAILING_WAGE columns had a missing value before the missing values were replcaced with another value.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    if row['WAGE_RATE_OF_PAY_TO']=='0.00':\n",
    "        df.at[index, 'WAGE_RATE_OF_PAY_TO'] = row['WAGE_RATE_OF_PAY_FROM']\n",
    "        \n",
    "df['MAX_WAGE_INDICATOR'] = df['WAGE_RATE_OF_PAY_TO'].isnull().astype(int)    \n",
    "missing_maxwage=df[df['WAGE_RATE_OF_PAY_TO'].isnull()] #The offered wage for the entry with a maximum wage value missing is 63,024.00\n",
    "WAGE_RATE_OF_PAY_FROM='63,024.00'\n",
    "df['WAGE_RATE_OF_PAY_TO'].fillna(value=WAGE_RATE_OF_PAY_FROM, inplace=True)\n",
    "\n",
    "df['PREVAILING_WAGE_INDICATOR']=df['PREVAILING_WAGE'].isnull().astype(int)   \n",
    "df['PREVAILING_WAGE'].fillna(value='0.00', inplace=True)\n",
    " \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install plotly graphical visualization tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install plotly --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code creates a scatter plot showing the relationship between the wage offered by the employer to the prevailing wage for the position.  Any wages that seem unreasonable will have the rows they are contained in removed.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import numpy as np\n",
    "from key import key\n",
    "import plotly\n",
    "from IPython.display import Image\n",
    "from IPython.display import display\n",
    "\n",
    "plotly.tools.set_credentials_file(username='spaturi', api_key=key)\n",
    "\n",
    "trace = go.Scattergl(\n",
    "    x = np.array(pw_year),    #Graph the prevailing wage with units year\n",
    "    y = np.array(wage_year),  #Graph the wage offered with units year\n",
    "    mode = 'markers',\n",
    "    marker = dict(\n",
    "        color = '#FFBAD2',\n",
    "        line = dict(width = 1)\n",
    "    )\n",
    ")\n",
    "layout = go.Layout(\n",
    "    title='Annual Wage Offered by Employer vs. Annual Prevailing Wage',\n",
    "    xaxis=dict(\n",
    "        title='Prevailing Wage in Dollars',\n",
    "        titlefont=dict(\n",
    "            family='Courier New, monospace',\n",
    "            size=18,\n",
    "            color='#7f7f7f'\n",
    "        )\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title='Wage Offered by Employer in Dollars',\n",
    "        titlefont=dict(\n",
    "            family='Courier New, monospace',\n",
    "            size=18,\n",
    "            color='#7f7f7f'\n",
    "        )\n",
    "    )\n",
    ")\n",
    "fig = go.Figure(data=[trace], layout=layout)\n",
    "\n",
    "#py.iplot(fig)\n",
    "Image(\"Annual_Wage_Graph.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear from the graph above that there are outlier values in the prevailing wage axis.  There are some entries with a prevailing wage greater than 50 million.  These are not correct prevailing wages and should be removed from the data set.  However, when looking at the case status of these applications with a prevailing wage greater than 50 million below, it is seen that all the application are denied.  It can be hypothesized that an unrealistic prevailing wage listed can lead to an application denial.  Removing these rows would remove rows that recieved a denial status and make the data set even more unbalanced towards the certified entries.  Therefore, the outliers are kept because prevailing wage may be a large factor in the application case status.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    if row['PW_UNIT_OF_PAY']=='Year':\n",
    "        if float(row['PREVAILING_WAGE'])>50000000:\n",
    "            print(row['PREVAILING_WAGE'], row['JOB_TITLE'], row['CASE_STATUS'])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code cell displays the final dataframe after the data wrangling steps have concluded.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accepter=df[df['CASE_STATUS']!='DENIED']\n",
    "denier=df[df['CASE_STATUS']=='DENIED']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('cleaned.csv', mode = 'w', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
